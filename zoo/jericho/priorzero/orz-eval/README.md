# ORZ Evaluator - ç‹¬ç«‹è¯„ä¼°æ¨¡å—

ç”¨äºè¯„ä¼°å·²è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨æ•°å­¦å’Œæ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ¨¡å—æ”¯æŒå¤šç§æ•°æ®é›†å’Œçµæ´»çš„é…ç½®é€‰é¡¹ã€‚

## ğŸ“‹ åŠŸèƒ½ç‰¹æ€§

- âœ… æ”¯æŒå¤šä¸ªè¯„ä¼°æ•°æ®é›†ï¼ˆMath500ã€AIME2024ã€GPQA Diamondã€Jerichoï¼‰
- âœ… é›†æˆ vLLM è¿›è¡Œé«˜æ•ˆæ¨ç†
- âœ… æ”¯æŒåˆ†å¸ƒå¼æ¨ç†ï¼ˆå¤š GPUï¼‰
- âœ… çµæ´»çš„å‚æ•°é…ç½®ï¼Œæ”¯æŒç®€æ´å’Œè¯¦ç»†ç”¨æ³•
- âœ… è‡ªåŠ¨ç­”æ¡ˆæå–å’Œæ­£ç¡®æ€§åˆ¤æ–­
- âœ… è¯¦ç»†çš„è¯„ä¼°ç»“æœä¿å­˜ï¼ˆJSONL æ ¼å¼ï¼‰
- âœ… æ”¯æŒä¼ å…¥å·²åŠ è½½çš„æ¨¡å‹æˆ–é¢„åŠ è½½æ¨¡å‹å¯¹è±¡

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install transformers vllm ray loguru
```

### åŸºæœ¬ç”¨æ³•

#### æ–¹å¼ 1ï¼šç®€æ´ç”¨æ³•ï¼ˆæ¨èï¼‰

```python
from eval_orz import Evaluator
import asyncio

# åªéœ€ä¼ å…¥æ¨¡å‹è·¯å¾„å’Œæ•°æ®é›†è·¯å¾„
evaluator = Evaluator(
    model_path="path/to/model/checkpoint",
    eval_prompt_data=[
        "data/eval_data/math500.json",
        "data/eval_data/aime2024.json",
        "data/eval_data/gpqa_diamond.json",
    ]
)

# è¿è¡Œè¯„ä¼°
results = asyncio.run(evaluator.eval())
print(f"è¯„ä¼°ç»“æœ: {results}")

# æ¸…ç†èµ„æº
evaluator.cleanup()
```

#### æ–¹å¼ 2ï¼šè¦†ç›–é»˜è®¤å‚æ•°

```python
# ä¼ å…¥å¿…è¦å‚æ•°ï¼ŒåŒæ—¶è¦†ç›–éƒ¨åˆ†é…ç½®
evaluator = Evaluator(
    model_path="path/to/model",
    eval_prompt_data=["data/eval_data/math500.json"],
    temperature=0.8,  # è¦†ç›–é»˜è®¤å€¼ 1.0
    gpu_memory_utilization=0.5,  # è¦†ç›–é»˜è®¤å€¼ 0.3
    vllm_num_engines=2,  # å¤š GPU æ—¶å¯å¢åŠ 
)

results = asyncio.run(evaluator.eval())
```

#### æ–¹å¼ 3ï¼šå®Œæ•´çš„ Config å¯¹è±¡ï¼ˆé«˜çº§ï¼‰

```python
from eval_orz import EvaluatorConfig, Evaluator

config = EvaluatorConfig(
    model_path="path/to/model",
    tokenizer_path="path/to/model",  # å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ model_path
    vllm_num_engines=1,
    vllm_tensor_parallel_size=1,
    enable_prefix_caching=True,
    gpu_memory_utilization=0.3,
    max_model_len=8192,
    temperature=1.0,
    top_p=1.0,
    top_k=-1,
    generate_max_len=8000,
    stop=["User:", "Human:", "Assistant:", "</answer>"],
    eval_prompt_data=[
        "data/eval_data/eval_jericho_dataset_his10_4games_1.8k_20251013_instruct.json",
        "data/eval_data/math500.json",
        "data/eval_data/aime2024.json",
        "data/eval_data/gpqa_diamond.json",
    ],
    prompt_max_len=2048,
    output_dir="eval_results",
    save_detailed_results=True,
)

evaluator = Evaluator(config)
results = asyncio.run(evaluator.eval())
```

#### æ–¹å¼ 4ï¼šä¼ å…¥å·²åŠ è½½çš„æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# é¢„åŠ è½½æ¨¡å‹å’Œ tokenizer
model = AutoModelForCausalLM.from_pretrained("path/to/model")
tokenizer = AutoTokenizer.from_pretrained("path/to/model")

# ä¼ å…¥é¢„åŠ è½½çš„å¯¹è±¡
evaluator = Evaluator(
    model=model,
    tokenizer=tokenizer,
    eval_prompt_data=["data/eval_data/math500.json"]
)

results = asyncio.run(evaluator.eval())
```

## ğŸ“Š è¯„ä¼°æ•°æ®é›†

æ”¯æŒä»¥ä¸‹æ•°æ®é›†ï¼ˆJSON æ ¼å¼ï¼‰ï¼š

| æ•°æ®é›† | æ–‡ä»¶å | æè¿° |
|-------|--------|------|
| Math500 | `math500.json` | 500 ä¸ªæ•°å­¦é—®é¢˜ |
| AIME2024 | `aime2024.json` | 2024 å¹´ AIME ç«èµ›é¢˜ |
| GPQA Diamond | `gpqa_diamond.json` | é«˜éš¾åº¦é€šç”¨çŸ¥è¯†é¢˜ |
| Jericho | `eval_jericho_dataset_his10_4games_1.8k_20251013_instruct.json` | æ–‡æœ¬å†’é™©æ¸¸æˆæ•°æ® |

æ•°æ®é›†æ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {
    "prompt": [{"value": "é—®é¢˜å†…å®¹"}],
    "final_answer": "æœŸæœ›ç­”æ¡ˆ",
    "file_name": "source_dataset"
  }
]
```

## âš™ï¸ å‚æ•°è¯´æ˜

### æ¨¡å‹å’Œåˆ†è¯å™¨é…ç½®

- `model_path` (str): æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹åç§°
- `tokenizer_path` (str, å¯é€‰): åˆ†è¯å™¨è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ `model_path`

### vLLM æ¨ç†é…ç½®

- `vllm_num_engines` (int): vLLM å¼•æ“æ•°é‡ï¼Œé»˜è®¤ 1ï¼ˆå¤š GPU å¯å¢åŠ ï¼‰
  - **æ³¨æ„**ï¼šå• GPU ç¯å¢ƒä¿æŒä¸º 1ï¼›å¤šèŠ‚ç‚¹è®­ç»ƒå¯å¢åŠ 
- `vllm_tensor_parallel_size` (int): å¼ é‡å¹¶è¡Œå¤§å°ï¼Œé»˜è®¤ 1
- `enable_prefix_caching` (bool): å¯ç”¨å‰ç¼€ç¼“å­˜ï¼Œé»˜è®¤ True
- `gpu_memory_utilization` (float): GPU å†…å­˜ä½¿ç”¨æ¯”ä¾‹ï¼ŒèŒƒå›´ [0.0-1.0]ï¼Œé»˜è®¤ 0.3
- `max_model_len` (int): æœ€å¤§æ¨¡å‹é•¿åº¦ï¼Œé»˜è®¤ 8192

### ç”Ÿæˆé…ç½®

- `temperature` (float): é‡‡æ ·æ¸©åº¦ï¼Œé»˜è®¤ 1.0
- `top_p` (float): nucleus é‡‡æ ·å‚æ•°ï¼Œé»˜è®¤ 1.0
- `top_k` (int): top-k é‡‡æ ·å‚æ•°ï¼Œé»˜è®¤ -1ï¼ˆç¦ç”¨ï¼‰
- `generate_max_len` (int): ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ 8000
- `stop` (List[str]): åœæ­¢è¯åˆ—è¡¨

### æ•°æ®å’Œè¾“å‡ºé…ç½®

- `eval_prompt_data` (List[str]): è¯„ä¼°æ•°æ®é›†è·¯å¾„åˆ—è¡¨
- `prompt_max_len` (int): æç¤ºè¯æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ 2048
- `output_dir` (str): è¾“å‡ºç»“æœç›®å½•ï¼Œé»˜è®¤ "eval_results"
- `save_detailed_results` (bool): æ˜¯å¦ä¿å­˜è¯¦ç»†ç»“æœï¼Œé»˜è®¤ True

## ğŸ“ˆ è¾“å‡ºç»“æœ

### æ§åˆ¶å°è¾“å‡º
```
Evaluation completed: math500/accuracy: 0.7500, aime2024/accuracy: 0.5200, gpqa_diamond/accuracy: 0.4800, eval_accuracy: 0.5833
```

### æ–‡ä»¶è¾“å‡º (eval_results/*.jsonl)
```json
{
  "prompt": "å®Œæ•´çš„æç¤ºè¯æ–‡æœ¬",
  "output": "æ¨¡å‹å®Œæ•´ç”Ÿæˆå†…å®¹",
  "final_answer": "\\boxed{ç­”æ¡ˆ}",
  "answer": "æœŸæœ›ç­”æ¡ˆ",
  "iscorrect": true
}
```

## ğŸ”§ å¯¹é½è¯´æ˜

æœ¬æ¨¡å—å‚æ•°å·²å¯¹é½åˆ° `Open-Reasoner-Zero/playground/orz_7b_ppo_jericho_1013.py`ï¼š

- âœ… vLLM é…ç½®ï¼šå®Œå…¨å¯¹åº”
- âœ… ç”Ÿæˆå‚æ•°ï¼šå®Œå…¨å¯¹åº”
- âœ… æ•°æ®é›†ï¼šåŒ…å« Jericho è¯„ä¼°æ•°æ®
- âš ï¸ `vllm_num_engines`ï¼š
  - å‚è€ƒå€¼ä¸º 8ï¼ˆå¤šèŠ‚ç‚¹ç¯å¢ƒï¼‰
  - å• GPU ç¯å¢ƒæ”¹ä¸º 1
  - å¤š GPU å¯æ ¹æ®éœ€è¦å¢åŠ 

## ğŸ“ å®Œæ•´ç¤ºä¾‹è„šæœ¬

```python
import asyncio
from eval_orz import Evaluator

async def main():
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = Evaluator(
        model_path="checkpoints/orz_0p5b_ppo_jericho_1012_1gpu/iter12/policy",
        eval_prompt_data=[
            "data/eval_data/eval_jericho_dataset_his10_4games_1.8k_20251013_instruct.json",
            "data/eval_data/math500.json",
            "data/eval_data/aime2024.json",
            "data/eval_data/gpqa_diamond.json",
        ]
    )

    try:
        # è¿è¡Œè¯„ä¼°
        results = await evaluator.eval()

        # å¤„ç†ç»“æœ
        print("=" * 50)
        print("è¯„ä¼°ç»“æœæ±‡æ€»:")
        for dataset, accuracy in results.items():
            if "accuracy" in dataset:
                print(f"  {dataset}: {accuracy:.4f}")

    finally:
        # æ¸…ç†èµ„æº
        evaluator.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: å¦‚ä½•åœ¨å¤š GPU ä¸ŠåŠ é€Ÿè¯„ä¼°ï¼Ÿ

A: å¢åŠ  `vllm_num_engines` å‚æ•°ï¼š
```python
evaluator = Evaluator(
    model_path="...",
    vllm_num_engines=2,  # ä½¿ç”¨ 2 ä¸ª vLLM å¼•æ“
)
```

### Q: å¦‚ä½•è°ƒæ•´ GPU å†…å­˜ä½¿ç”¨ï¼Ÿ

A: ä¿®æ”¹ `gpu_memory_utilization` å‚æ•°ï¼š
```python
evaluator = Evaluator(
    model_path="...",
    gpu_memory_utilization=0.5,  # ä½¿ç”¨ 50% GPU å†…å­˜
)
```

### Q: ç­”æ¡ˆæå–ä¸æ­£ç¡®ï¼Ÿ

A: æ£€æŸ¥æ•°æ®é›†æ ¼å¼ï¼Œç¡®ä¿ç­”æ¡ˆç”¨ `\boxed{}` æ ‡è®°ï¼š
```json
{
  "final_answer": "\\boxed{42}",
  ...
}
```

## ğŸ“š ç›¸å…³æ–‡ä»¶

- `eval_orz.py` - ä¸»è¯„ä¼°æ¨¡å—
- `dataset/eval_dataset.py` - è¯„ä¼°æ•°æ®é›†å¤„ç†
- `orz/ppo/tools/math_utils.py` - æ•°å­¦ç­”æ¡ˆéªŒè¯å·¥å…·

## ğŸ“„ è®¸å¯è¯

MIT License
