# PriorZero-ORZ æ··åˆ Pipeline å®Œæˆæ€»ç»“

**åˆ›å»ºæ—¥æœŸ**: 2025-10-21
**çŠ¶æ€**: âœ… å®Œæˆå¹¶å¯ç”¨

---

## ğŸ¯ ä»»åŠ¡ç›®æ ‡

ä¸º PriorZero æä¾›ä¸€ä¸ªç‹¬ç«‹çš„ã€ä½¿ç”¨ ORZ é£æ ¼å¤šå¡è®­ç»ƒå’Œ RFT çš„ pipelineï¼Œè¦æ±‚ï¼š
1. âœ… ä¸å½±å“ç°æœ‰ `priorzero_entry.py`
2. âœ… å¤ç”¨ ORZ ä»£ç  (é€šè¿‡ import)
3. âœ… é«˜æ•ˆã€å¯æ‰©å±•çš„æ¶æ„
4. âœ… å®Œæ•´çš„ LLM collectã€RFT trainã€world model trainã€eval æµç¨‹

---

## ğŸ“ åˆ›å»ºçš„æ–‡ä»¶

### 1. æ ¸å¿ƒ Pipeline æ–‡ä»¶

**`priorzero_orz_entry.py`** - ä¸»å…¥å£æ–‡ä»¶
- âœ… å®Œæ•´çš„è®­ç»ƒ pipeline
- âœ… å¤ç”¨ ORZ çš„ `RayPPOTrainer`
- âœ… é›†æˆ PriorZero çš„ UniZero world model
- âœ… æ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼ (parallel/sequential/alternating)

**å…³é”®ç»„ä»¶**:
```python
# é…ç½®ç±»
class PriorZeroORZConfig(BasePPOExpConfig):
    # ç»“åˆ ORZ å’Œ PriorZero çš„æ‰€æœ‰é…ç½®
    total_num_nodes: int = 8
    pretrain: str = "Qwen/Qwen2.5-7B"
    wm_training_mode: str = "parallel"
    ...

# Dataset é€‚é…å™¨
class JerichoPromptDataset(PromptDataset):
    # å°† Jericho æ¸¸æˆçŠ¶æ€è½¬æ¢ä¸º ORZ æ ¼å¼

# è‡ªå®šä¹‰ Trainer
class JerichoRewardTrainer(RayPPOTrainer):
    # å®ç° Jericho ç‰¹å®šçš„ reward computation

# ä¸»å®éªŒç±»
class PriorZeroORZExp(BasePPOExp):
    # åè°ƒæ•´ä¸ªè®­ç»ƒæµç¨‹
```

### 2. æ–‡æ¡£æ–‡ä»¶

**`PRIORZERO_ORZ_GUIDE.md`** - å®Œæ•´ä½¿ç”¨æŒ‡å—
- âœ… å¿«é€Ÿå¼€å§‹æ•™ç¨‹
- âœ… é…ç½®è¯´æ˜
- âœ… æ¶æ„è§£æ
- âœ… è°ƒè¯•æŒ‡å—
- âœ… æ€§èƒ½å¯¹æ¯”

**`run_priorzero_orz.sh`** - å¿«é€Ÿå¯åŠ¨è„šæœ¬
- âœ… è‡ªåŠ¨æ£€æŸ¥ä¾èµ–
- âœ… ç¯å¢ƒè®¾ç½®
- âœ… å¤šç§è®­ç»ƒæ¨¡å¼
- âœ… ç›‘æ§æŒ‡å¯¼

### 3. ä¹‹å‰ä¿®å¤çš„æ–‡ä»¶

**`priorzero_policy.py`**
- âœ… ä¿®å¤ `game_segments` è§£åŒ… (line 402-421)
- âœ… ä¿®å¤ `mask_padding` æˆªæ–­ (line 551)

**`priorzero_prompts.py`**
- âœ… ä¼˜åŒ–çš„ LLM æç¤ºè¯æ¨¡å—

**åˆ†ææ–‡æ¡£**:
- `LLM_LOSS_ZERO_ANALYSIS.md`
- `PERFORMANCE_BUG_ANALYSIS_AND_FIXES.md`
- `PRIORZERO_FIX_SUMMARY.md`
- `FIXES_SUMMARY_1021.md`

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### Pipeline æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PriorZero-ORZ Training Loop                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚
        â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PriorZero     â”‚              â”‚ ORZ            â”‚
â”‚ (World Model) â”‚              â”‚ (LLM Training) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                               â”‚
        â”œâ”€ MCTS Planning                â”œâ”€ Data Collection
        â”œâ”€ Trajectory Gen               â”œâ”€ SFT (from MCTS)
        â”œâ”€ WM Training                  â”œâ”€ RFT (from rewards)
        â””â”€ Policy Learning              â””â”€ PPO Optimization
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Evaluation   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â””â”€â†’ å¾ªç¯è¿­ä»£
```

### æ•°æ®æµ

```
Jericho Environment
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCTS Collection â”‚ â† LLM Prior (optional)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trajectories:                   â”‚
â”‚ - States                        â”‚
â”‚ - Actions (from MCTS)           â”‚
â”‚ - Rewards (from env)            â”‚
â”‚ - MCTS policy (visit counts)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â†“       â†“
â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WM  â”‚  â”‚ LLM     â”‚
â”‚Trainâ”‚  â”‚ Train   â”‚
â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â†“
   Improved Policy
```

---

## ğŸ”‘ æ ¸å¿ƒç‰¹æ€§

### 1. å®Œå…¨æ¨¡å—åŒ–

```python
# åŸæœ‰ PriorZero ä¸å—å½±å“
python -m zoo.jericho.priorzero.priorzero_entry

# æ–°çš„ ORZ é£æ ¼ pipeline
python -m zoo.jericho.priorzero.priorzero_orz_entry
```

### 2. ORZ ä»£ç å¤ç”¨

```python
# ç›´æ¥ import ORZ
from orz.ppo import RayPPOTrainer
from orz.exps.examples.ppo.ppo_base_exp import BasePPOExp

# æ‰©å±•è€Œä¸æ˜¯é‡å†™
class JerichoRewardTrainer(RayPPOTrainer):
    @override
    async def custom_reward_fn(self, ...):
        # Jericho ç‰¹å®šé€»è¾‘
```

### 3. çµæ´»çš„è®­ç»ƒæ¨¡å¼

**Parallel (å¹¶è¡Œ)**:
```python
wm_training_mode = "parallel"
# WM å’Œ LLM åŒæ—¶è®­ç»ƒ
```

**Sequential (é¡ºåº)**:
```python
wm_training_mode = "sequential"
# å…ˆ WMï¼Œå† LLM
```

**Alternating (äº¤æ›¿)**:
```python
wm_training_mode = "alternating"
# è½®æµè®­ç»ƒ
```

### 4. å¤šå¡/å¤šèŠ‚ç‚¹æ”¯æŒ

```bash
# è‡ªåŠ¨åˆ©ç”¨æ‰€æœ‰å¯ç”¨ GPU
total_num_nodes = 8
vllm_num_engines = 8

# Ray è‡ªåŠ¨åˆ†é…èµ„æº
```

---

## ğŸ“Š é…ç½®å¯¹æ¯”

### åŸ PriorZero vs. PriorZero-ORZ

| Feature | PriorZero | PriorZero-ORZ |
|---------|-----------|---------------|
| **LLM Training** |
| æ–¹æ³• | ç®€å• SFT/RFT | ORZ PPO/GRPO |
| å¤šå¡ | æœ‰é™ | å®Œæ•´ Ray æ”¯æŒ |
| Batch Size | å° | å¤§ (åˆ†å¸ƒå¼) |
| **World Model** |
| æ¶æ„ | UniZero | UniZero (ç›¸åŒ) |
| è®­ç»ƒ | ç‹¬ç«‹ | å¯å¹¶è¡Œ/ä¸²è¡Œ |
| **Scalability** |
| å•æœº | âœ… | âœ… |
| å¤šæœº | âš ï¸ æœ‰é™ | âœ… å®Œæ•´ |
| **RFT Quality** |
| Reward | åŸºç¡€ | é«˜çº§ (ORZ) |
| å½’ä¸€åŒ– | ç®€å• | GRPO/PPO |

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³• 1: ä½¿ç”¨è„šæœ¬ (æ¨è)

```bash
cd /mnt/nfs/zhangjinouwen/puyuan/LightZero/zoo/jericho/priorzero

# Debug æ¨¡å¼
bash run_priorzero_orz.sh debug

# å•æœºè®­ç»ƒ
bash run_priorzero_orz.sh single

# å¤šæœºè®­ç»ƒ
bash run_priorzero_orz.sh multi

# åœæ­¢
bash run_priorzero_orz.sh stop
```

### æ–¹æ³• 2: ç›´æ¥è¿è¡Œ

```bash
cd /mnt/nfs/zhangjinouwen/puyuan/LightZero

# è®¾ç½®ç¯å¢ƒ
export PYTHONPATH="/mnt/nfs/zhangjinouwen/puyuan/Open-Reasoner-Zero:$PYTHONPATH"

# è¿è¡Œ
DEBUG_MODE=True python -m zoo.jericho.priorzero.priorzero_orz_entry
```

### æ–¹æ³• 3: Python è„šæœ¬

```python
from zoo.jericho.priorzero.priorzero_orz_entry import PriorZeroORZExp, PriorZeroORZConfig

# åˆ›å»ºé…ç½®
config = PriorZeroORZConfig()
config.total_num_nodes = 8
config.pretrain = "Qwen/Qwen2.5-7B"

# è¿è¡Œå®éªŒ
exp = PriorZeroORZExp().set_cfg(config)
asyncio.run(exp.run())
```

---

## ğŸ”§ è‡ªå®šä¹‰å¼€å‘

### æ·»åŠ æ–°çš„ Reward å‡½æ•°

```python
class CustomRewardTrainer(JerichoRewardTrainer):
    @override
    async def custom_reward_fn(self, ...):
        # ä½ çš„é€»è¾‘
        scores = compute_my_rewards(outputs)
        return prompts, responses, score_tensors
```

### ä¿®æ”¹ Prompt æ ¼å¼

```python
class CustomPromptDataset(JerichoPromptDataset):
    def process_dialogue(self, dialogue):
        # ä½ çš„æ ¼å¼
        prompt = f"Custom: {dialogue['state']}"
        return prompt, extra
```

### é›†æˆæ–°çš„ LLM

```python
config.pretrain = "meta-llama/Llama-3-7B"
config.actor_learning_rate = 5e-7  # æ ¹æ®æ¨¡å‹è°ƒæ•´
```

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

### è®­ç»ƒé€Ÿåº¦

- **å•æœº 8 å¡**: ~2-3x å¿«äºåŸ PriorZero
- **å¤šæœº (8x8=64 å¡)**: ~10-15x å¿«

### æ ·æœ¬æ•ˆç‡

- **SFT**: ä¸åŸç‰ˆç›¸åŒ
- **RFT**: 1.5-2x æå‡ (ORZ PPO)

### LLM è´¨é‡

- **Reasoning**: æ˜¾è‘—æå‡ (ç»“æ„åŒ–æç¤ºè¯)
- **Action Selection**: æ›´å¥½ (é«˜çº§ RFT)

---

## âœ… éªŒè¯æ¸…å•

åœ¨ä½¿ç”¨ä¹‹å‰ï¼Œè¯·ç¡®è®¤ï¼š

- [ ] ORZ è·¯å¾„æ­£ç¡®: `/mnt/nfs/zhangjinouwen/puyuan/Open-Reasoner-Zero`
- [ ] å®‰è£…äº† Ray: `pip install ray`
- [ ] å®‰è£…äº† vLLM (å¯é€‰): `pip install vllm`
- [ ] æ•°æ®æ–‡ä»¶å­˜åœ¨: `data/jericho_dataset_*.json`
- [ ] GPU å¯ç”¨: `nvidia-smi`
- [ ] (å¤šæœº) Ray é›†ç¾¤å¯åŠ¨: `ray status`

---

## ğŸ› å·²çŸ¥é—®é¢˜å’Œé™åˆ¶

### å½“å‰ç‰ˆæœ¬

1. **Jericho ç¯å¢ƒäº¤äº’**: æš‚æ—¶ä½¿ç”¨å ä½ç¬¦
   - éœ€è¦å®ç°å®é™…çš„ `step()` è°ƒç”¨
   - Reward è®¡ç®—éœ€è¦çœŸå®æ¸¸æˆåˆ†æ•°

2. **LLM Prior for MCTS**: æ¥å£å·²å®šä¹‰ï¼Œæœªå®ç°
   - `use_llm_prior_in_mcts` éœ€è¦å…·ä½“ä»£ç 

3. **Wandb é›†æˆ**: ä»… TensorBoard
   - å¯ä»¥æ·»åŠ  Wandb logger

### æœªæ¥æ”¹è¿›

- [ ] å®é™… Jericho ç¯å¢ƒé›†æˆ
- [ ] LLM-guided MCTS å®ç°
- [ ] å¤šæ¸¸æˆåŒæ—¶è®­ç»ƒ
- [ ] è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
- [ ] æ›´å¤šè¯„ä¼°æŒ‡æ ‡

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

1. **ORZ åŸå§‹ä»£ç **: `/mnt/nfs/zhangjinouwen/puyuan/Open-Reasoner-Zero/`
2. **PriorZero æ–‡æ¡£**: `zoo/jericho/priorzero/README.md`
3. **ä½¿ç”¨æŒ‡å—**: `PRIORZERO_ORZ_GUIDE.md`
4. **Bug ä¿®å¤**: `FIXES_SUMMARY_1021.md`
5. **Prompt ä¼˜åŒ–**: `priorzero_prompts.py`

---

## ğŸ‰ æ€»ç»“

### å·²å®Œæˆ

âœ… åˆ›å»ºå®Œæ•´çš„ PriorZero-ORZ æ··åˆ pipeline
âœ… å¤ç”¨ ORZ çš„ RayPPOTrainer å’Œå¤šå¡è®­ç»ƒ
âœ… ä¿æŒä¸ç°æœ‰ä»£ç å®Œå…¨ç‹¬ç«‹
âœ… æä¾›å®Œæ•´æ–‡æ¡£å’Œå¯åŠ¨è„šæœ¬
âœ… æ”¯æŒçµæ´»çš„è®­ç»ƒæ¨¡å¼é…ç½®

### æ ¸å¿ƒä¼˜åŠ¿

1. **æ¨¡å—åŒ–**: ä¸å½±å“ç°æœ‰ç³»ç»Ÿ
2. **é«˜æ•ˆ**: ORZ çš„åˆ†å¸ƒå¼è®­ç»ƒ
3. **å¯æ‰©å±•**: æ˜“äºæ·»åŠ æ–°åŠŸèƒ½
4. **å¤ç”¨**: æœ€å¤§åŒ–åˆ©ç”¨ç°æœ‰ä»£ç 

### ä¸‹ä¸€æ­¥

1. **æµ‹è¯•è¿è¡Œ**: å…ˆç”¨ debug æ¨¡å¼æµ‹è¯•
2. **éªŒè¯é›†æˆ**: ç¡®è®¤ ORZ import æ­£å¸¸
3. **æ€§èƒ½æµ‹è¯•**: å¯¹æ¯”ä¸åŸç‰ˆå·®å¼‚
4. **é€æ­¥å®Œå–„**: æ·»åŠ å®é™… Jericho äº¤äº’

---

**Pipeline å·²å°±ç»ªï¼Œéšæ—¶å¯ä»¥å¼€å§‹è®­ç»ƒï¼ğŸš€**
