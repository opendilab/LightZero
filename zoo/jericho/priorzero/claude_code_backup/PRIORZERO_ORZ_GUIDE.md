# PriorZero-ORZ Hybrid Pipeline ä½¿ç”¨æŒ‡å—

**æ–‡ä»¶**: `priorzero_orz_entry.py`
**åˆ›å»ºæ—¥æœŸ**: 2025-10-21

---

## ğŸ¯ æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„è®­ç»ƒ pipelineï¼Œç»“åˆäº†:
- **PriorZero**: UniZero world model + MCTS è§„åˆ’
- **ORZ**: åˆ†å¸ƒå¼ LLM è®­ç»ƒ (PPO/GRPO + RFT)

### å…³é”®ç‰¹æ€§

1. âœ… **å®Œå…¨æ¨¡å—åŒ–**: ä¸å½±å“ç°æœ‰ `priorzero_entry.py`
2. âœ… **å¤ç”¨ ORZ ä»£ç **: ç›´æ¥ import ORZ çš„ `RayPPOTrainer`
3. âœ… **å¤šå¡/å¤šèŠ‚ç‚¹**: æ”¯æŒ Ray åˆ†å¸ƒå¼è®­ç»ƒ
4. âœ… **é«˜æ•ˆ MCTS**: ç”¨äºç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®
5. âœ… **å…ˆè¿› RFT**: ORZ çš„ reward computation

---

## ğŸ“¦ ä¾èµ–å®‰è£…

### 1. ç¡®ä¿ ORZ å¯è®¿é—®

```bash
# æ£€æŸ¥ ORZ è·¯å¾„
ls /mnt/nfs/zhangjinouwen/puyuan/Open-Reasoner-Zero

# å¦‚æœéœ€è¦ï¼Œæ·»åŠ åˆ° PYTHONPATH
export PYTHONPATH="/mnt/nfs/zhangjinouwen/puyuan/Open-Reasoner-Zero:$PYTHONPATH"
```

### 2. å®‰è£…ä¾èµ–

```bash
# ORZ ä¾èµ–
pip install ray vllm loguru jinja2

# PriorZero ä¾èµ–
pip install jericho transformers torch
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å•æœºè®­ç»ƒ

```bash
cd /mnt/nfs/zhangjinouwen/puyuan/LightZero

# è°ƒè¯•æ¨¡å¼ (å°è§„æ¨¡)
DEBUG_MODE=True python -m zoo.jericho.priorzero.priorzero_orz_entry

# æ­£å¸¸è®­ç»ƒ
python -m zoo.jericho.priorzero.priorzero_orz_entry
```

### å¤šèŠ‚ç‚¹è®­ç»ƒ

```bash
# èŠ‚ç‚¹ 1 (master): å¯åŠ¨ Ray
ray start --head --port=6379

# èŠ‚ç‚¹ 2-N: è¿æ¥åˆ° master
ray start --address='<master-ip>:6379'

# èŠ‚ç‚¹ 1: è¿è¡Œè®­ç»ƒ
python -m zoo.jericho.priorzero.priorzero_orz_entry
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### `PriorZeroORZConfig` ä¸»è¦å‚æ•°

#### LLM è®­ç»ƒ (ORZ)

```python
# æ¨¡å‹
pretrain: str = "Qwen/Qwen2.5-7B"

# èµ„æºåˆ†é…
total_num_nodes: int = 8          # æ€»èŠ‚ç‚¹æ•°
vllm_num_engines: int = 8         # vLLM å¼•æ“æ•°
colocate_all: bool = True         # æ‰€æœ‰ç»„ä»¶åŒä½

# PPO è®­ç»ƒ
actor_learning_rate: float = 1e-6
critic_learning_rate: float = 5e-6
rollout_batch_size: int = 128
n_samples_per_prompt: int = 64
policy_update_steps: int = 1
critic_update_steps: int = 12

# ç”Ÿæˆ
generate_max_len: int = 8000
temperature: float = 1.0
top_p: float = 1.0
```

#### World Model è®­ç»ƒ (PriorZero)

```python
# ç¯å¢ƒ
env_id: str = 'zork1.z5'

# è®­ç»ƒ
wm_learning_rate: float = 3e-4
wm_batch_size: int = 32
wm_replay_buffer_size: int = 10000

# MCTS
num_simulations: int = 25
mcts_temperature: float = 1.0
```

#### æ··åˆæ¨¡å¼

```python
# è®­ç»ƒç­–ç•¥
wm_training_mode: str = "parallel"
# é€‰é¡¹: "parallel", "sequential", "alternating"

# WM å’Œ LLM è®­ç»ƒæ—¶é—´åˆ†é…
wm_llm_ratio: float = 0.5  # 0.5 = å„å ä¸€åŠ

# LLM prior ç”¨äº MCTS
use_llm_prior_in_mcts: bool = True
llm_prior_weight: float = 0.3
```

---

## ğŸ“Š Pipeline æ¶æ„

### è®­ç»ƒæµç¨‹

```
1. MCTS Data Collection (PriorZero)
   â”œâ”€ ä½¿ç”¨ world model è¿›è¡Œè§„åˆ’
   â”œâ”€ (å¯é€‰) LLM prior å¼•å¯¼æœç´¢
   â””â”€ ç”Ÿæˆ (state, action, reward, mcts_policy) è½¨è¿¹

2. World Model Training (PriorZero)
   â”œâ”€ è®­ç»ƒ UniZero transformer
   â””â”€ å­¦ä¹  dynamics, value, policy

3. LLM Training (ORZ)
   â”œâ”€ SFT: ç›‘ç£å¾®è°ƒ (ä» MCTS policy)
   â””â”€ RFT: å¼ºåŒ–å¾®è°ƒ (ä»ç¯å¢ƒ rewards)

4. Evaluation
   â”œâ”€ World model performance
   â””â”€ LLM policy quality
```

### æ•°æ®æµ

```
Jericho Env
    â†“
MCTS Planning â†’ Trajectories
    â†“                â†“
World Model    LLM Training (ORZ)
  (UniZero)      (RayPPOTrainer)
    â†“                â†“
Improved       Improved LLM
 MCTS         Policy Prior
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          å¾ªç¯è¿­ä»£
```

---

## ğŸ”§ å…³é”®ç»„ä»¶

### 1. `JerichoPromptDataset`

å°† Jericho æ¸¸æˆçŠ¶æ€è½¬æ¢ä¸º ORZ æ ¼å¼çš„æç¤ºè¯:

```python
# è¾“å…¥: Jericho æ¸¸æˆçŠ¶æ€
{
    "prompt": [{"value": "You are in a dark room..."}],
    "final_answer": "take lamp"
}

# è¾“å‡º: ORZ æç¤ºè¯
"""
A conversation between User and Assistant...
<think> reasoning </think> <answer> take lamp </answer>
"""
```

### 2. `JerichoRewardTrainer`

æ‰©å±• ORZ çš„ `RayPPOTrainer`ï¼Œå®ç° Jericho ç‰¹å®šçš„ reward:

```python
# Reward ç»“æ„:
- æ­£ç¡®åŠ¨ä½œ (+åˆ†æ•°): +1.0
- æœ‰æ•ˆåŠ¨ä½œ: +0.1
- æ— æ•ˆåŠ¨ä½œ: -0.5
- æ ¼å¼æ­£ç¡®: +0.1
```

### 3. `PriorZeroORZExp`

ä¸»å®éªŒç±»ï¼Œåè°ƒ world model å’Œ LLM è®­ç»ƒã€‚

---

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### TensorBoard

```bash
tensorboard --logdir=priorzero_orz_logs/ --port=6006
```

å…³é”®æŒ‡æ ‡:
- `train/llm/sft_loss` - LLM ç›‘ç£å¾®è°ƒæŸå¤±
- `train/llm/rft_loss` - LLM å¼ºåŒ–å¾®è°ƒæŸå¤±
- `train/wm/total_loss` - World model æ€»æŸå¤±
- `evals/reward_mean` - è¯„ä¼°å¹³å‡å¥–åŠ±

### æ—¥å¿—æ–‡ä»¶

```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f priorzero_orz_logs/*/log/*.log

# æœç´¢ LLM ç›¸å…³
grep "LLM" priorzero_orz_logs/*/log/*.log
```

---

## ğŸ›ï¸ è®­ç»ƒæ¨¡å¼

### Mode 1: Parallel (é»˜è®¤)

World model å’Œ LLM å¹¶è¡Œè®­ç»ƒ:

```python
wm_training_mode = "parallel"
```

- âœ… æœ€å¿«
- âš ï¸ éœ€è¦æ›´å¤š GPU å†…å­˜

### Mode 2: Sequential

å…ˆè®­ç»ƒ world modelï¼Œå†è®­ç»ƒ LLM:

```python
wm_training_mode = "sequential"
```

- âœ… å†…å­˜å‹å¥½
- âš ï¸ è®­ç»ƒæ—¶é—´æ›´é•¿

### Mode 3: Alternating

äº¤æ›¿è®­ç»ƒ:

```python
wm_training_mode = "alternating"
```

- âœ… å¹³è¡¡æ•ˆç‡å’Œå†…å­˜
- âœ… æ›´ç¨³å®šçš„å­¦ä¹ 

---

## ğŸ” è°ƒè¯•

### Debug æ¨¡å¼

```bash
DEBUG_MODE=True python -m zoo.jericho.priorzero.priorzero_orz_entry
```

è‡ªåŠ¨è°ƒæ•´:
- `total_num_nodes = 2`
- `rollout_batch_size = 16`
- `n_samples_per_prompt = 2`
- `num_episodes = 2`

### å¸¸è§é—®é¢˜

#### 1. Ray è¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥ Ray çŠ¶æ€
ray status

# é‡å¯ Ray
ray stop
ray start --head
```

#### 2. GPU å†…å­˜ä¸è¶³

```bash
# é™ä½ batch size
wm_batch_size = 16
rollout_batch_size = 64

# æˆ–ä½¿ç”¨ sequential æ¨¡å¼
wm_training_mode = "sequential"
```

#### 3. ORZ import å¤±è´¥

```bash
# æ·»åŠ åˆ° PYTHONPATH
export PYTHONPATH="/mnt/nfs/zhangjinouwen/puyuan/Open-Reasoner-Zero:$PYTHONPATH"

# æˆ–åœ¨ä»£ç ä¸­è®¾ç½® (å·²åŒ…å«åœ¨ entry æ–‡ä»¶ä¸­)
```

---

## ğŸ“ è‡ªå®šä¹‰å¼€å‘

### æ·»åŠ æ–°çš„ reward å‡½æ•°

ç¼–è¾‘ `JerichoRewardTrainer.custom_reward_fn()`:

```python
@override
async def custom_reward_fn(self, ...):
    # ä½ çš„ reward è®¡ç®—é€»è¾‘
    scores = compute_custom_rewards(outputs)
    return prompts, responses, score_tensors
```

### ä¿®æ”¹æç¤ºè¯æ ¼å¼

ç¼–è¾‘ `JerichoPromptDataset.process_dialogue()`:

```python
prompt_template_jinja = """
{{bos_token}}Your custom prompt template here...
"""
```

### æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡

åœ¨ `PriorZeroORZExp` ä¸­æ·»åŠ :

```python
@override
async def eval(self):
    # ä½ çš„è¯„ä¼°é€»è¾‘
    pass
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### vs. åŸ PriorZero (`priorzero_entry.py`)

| Feature | PriorZero | PriorZero-ORZ |
|---------|-----------|---------------|
| World Model | âœ… UniZero | âœ… UniZero |
| LLM Training | âŒ ç®€å• | âœ… ORZ PPO |
| Multi-GPU | âš ï¸ æœ‰é™ | âœ… Full Ray |
| RFT | âš ï¸ åŸºç¡€ | âœ… é«˜çº§ |
| Scalability | â­â­ | â­â­â­â­â­ |

### é¢„æœŸæå‡

- **è®­ç»ƒé€Ÿåº¦**: 2-3x (å¤šå¡å¹¶è¡Œ)
- **æ ·æœ¬æ•ˆç‡**: 1.5x (æ›´å¥½çš„ RFT)
- **LLM è´¨é‡**: æ˜¾è‘—æå‡ (ORZ PPO)

---

## ğŸ—ºï¸ è·¯çº¿å›¾

### å½“å‰ç‰ˆæœ¬ (v1.0)

- âœ… ORZ RayPPOTrainer é›†æˆ
- âœ… Jericho reward å‡½æ•°
- âœ… åŸºç¡€ prompt æ ¼å¼
- âœ… å¤šå¡è®­ç»ƒæ”¯æŒ

### è®¡åˆ’ä¸­ (v1.1)

- [ ] å®é™… Jericho ç¯å¢ƒäº¤äº’
- [ ] LLM prior æ•´åˆåˆ° MCTS
- [ ] é«˜çº§ reward shaping
- [ ] Wandb é›†æˆ

### æœªæ¥ (v2.0)

- [ ] å¤šæ¸¸æˆæ”¯æŒ
- [ ] Meta-learning
- [ ] è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **ORZ æ–‡æ¡£**: `/mnt/nfs/zhangjinouwen/puyuan/Open-Reasoner-Zero/README.md`
2. **PriorZero æ–‡æ¡£**: `zoo/jericho/priorzero/README.md`
3. **Ray æ–‡æ¡£**: https://docs.ray.io/

---

## ğŸ¤ è´¡çŒ®

å‘ç° bug æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Ÿ
1. æŸ¥çœ‹ç°æœ‰ä»£ç 
2. ä¿®æ”¹å¹¶æµ‹è¯•
3. æ–‡æ¡£æ›´æ–°

---

**Happy Training! ğŸš€**
