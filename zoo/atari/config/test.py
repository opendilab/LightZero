from easydict import EasyDict

main_config = dict(
    exp_name='data_unizero_atari_mt_balance_20250625/atari_8games_balance-total-stage5_stage-50k-20k_fix-lora-update-stablescale_vit-small-ln_moe8-lora_trans-nlayer4_brf1e-06_not-share-head_seed0/Pong_seed0',
    env=dict(
        manager=dict(
            episode_num=float('inf'),
            max_retry=1,
            step_timeout=None,
            auto_reset=True,
            reset_timeout=None,
            retry_type='reset',
            retry_waiting_time=0.1,
            shared_memory=False,
            copy_on_get=True,
            context='fork',
            wait_num=float('inf'),
            step_wait_timeout=None,
            connect_timeout=60,
            reset_inplace=False,
            cfg_type='SyncSubprocessEnvManagerDict',
            type='subprocess',
        ),
        stop_value=1000000,
        n_evaluator_episode=3,
        full_action_space=True,
        collector_env_num=8,
        evaluator_env_num=3,
        env_type='Atari',
        observation_shape=[3, 64, 64],
        collect_max_episode_steps=5000,
        eval_max_episode_steps=5000,
        render_mode_human=False,
        save_replay=False,
        replay_path=None,
        gray_scale=False,
        frame_stack_num=1,
        frame_skip=4,
        episode_life=True,
        clip_rewards=True,
        channel_last=False,
        scale=True,
        warp_frame=True,
        transform2string=False,
        game_wrapper=True,
        cfg_type='AtariEnvLightZeroDict',
        env_id='PongNoFrameskip-v4',
    ),
    policy=dict(
        model=dict(
            model_type='conv',
            continuous_action_space=False,
            observation_shape=[3, 64, 64],
            self_supervised_learning_loss=True,
            categorical_distribution=True,
            image_channel=3,
            frame_stack_num=1,
            num_res_blocks=2,
            num_channels=256,
            support_scale=50,
            bias=True,
            discrete_action_encoding_type='one_hot',
            res_connection_in_dynamics=True,
            norm_type='LN',
            analysis_sim_norm=False,
            analysis_dormant_ratio=False,
            harmony_balance=False,
            learn={'learner': {'hook': {'save_ckpt_after_iter': 10000}}},
            world_model_cfg={'continuous_action_space': False, 'tokens_per_block': 2, 'max_blocks': 10, 'max_tokens': 20, 'context_length': 8, 'gru_gating': False, 'device': 'cuda', 'analysis_sim_norm': False, 'analysis_dormant_ratio_weight_rank': False, 'action_space_size': 18, 'group_size': 8, 'attention': 'causal', 'num_layers': 4, 'num_heads': 24, 'embed_dim': 768, 'embed_pdrop': 0.1, 'resid_pdrop': 0.1, 'attn_pdrop': 0.1, 'support_size': 101, 'max_cache_size': 5000, 'env_num': 8, 'latent_recon_loss_weight': 0.0, 'perceptual_loss_weight': 0.0, 'policy_entropy_weight': 0.0001, 'predict_latent_loss_type': 'mse', 'obs_type': 'image', 'gamma': 1, 'dormant_threshold': 0.025, 'rotary_emb': False, 'rope_theta': 10000, 'max_seq_len': 8192, 'lora_r': 64, 'analysis_dormant_ratio': False, 'use_global_pooling': False, 'final_norm_option_in_obs_head': 'LayerNorm', 'final_norm_option_in_encoder': 'LayerNorm', 'share_head': False, 'task_embed_option': None, 'use_task_embed': False, 'use_shared_projection': False, 'task_num': 8, 'encoder_type': 'vit', 'use_normal_head': True, 'use_softmoe_head': False, 'use_moe_head': False, 'num_experts_in_moe_head': 4, 'moe_in_transformer': False, 'multiplication_moe_in_transformer': True, 'n_shared_experts': 1, 'num_experts_per_tok': 1, 'num_experts_of_moe_in_transformer': 8, 'moe_use_lora': False, 'curriculum_stage_num': 5, 'lora_target_modules': ['attn', 'feed_forward'], 'lora_alpha': 1, 'lora_dropout': 0.0, 'lora_scale_init': 1, 'min_stage0_iters': 50000, 'max_stage_iters': 20000},
            action_space_size=18,
        ),
        learn=dict(
            learner=dict(
                train_iterations=1000000000,
                dataloader=dict(
                    num_workers=0,
                ),
                log_policy=True,
                hook=dict(
                    load_ckpt_before_run='',
                    log_show_after_iter=100,
                    save_ckpt_after_iter=200000,
                    save_ckpt_after_run=True,
                ),
                cfg_type='BaseLearnerDict',
            ),
        ),
        collect=dict(
            collector=dict(
                deepcopy_obs=False,
                transform_obs=False,
                collect_print_freq=100,
                cfg_type='SampleSerialCollectorDict',
                type='sample',
            ),
        ),
        eval=dict(
            evaluator=dict(
                eval_freq=1000,
                render={'render_freq': -1, 'mode': 'train_iter'},
                figure_path=None,
                cfg_type='InteractionSerialEvaluatorDict',
                stop_value=1000000,
                n_episode=3,
            ),
        ),
        other=dict(
            replay_buffer=dict(
                type='advanced',
                replay_buffer_size=4096,
                max_use=float('inf'),
                max_staleness=float('inf'),
                alpha=0.6,
                beta=0.4,
                anneal_step=100000,
                enable_track_used_data=False,
                deepcopy=False,
                thruput_controller=dict(
                    push_sample_rate_limit=dict(
                        max=float('inf'),
                        min=0,
                    ),
                    window_seconds=30,
                    sample_min_limit_ratio=1,
                ),
                monitor=dict(
                    sampled_data_attr=dict(
                        average_range=5,
                        print_freq=200,
                    ),
                    periodic_thruput=dict(
                        seconds=60,
                    ),
                ),
                cfg_type='AdvancedReplayBufferDict',
            ),
        ),
        on_policy=False,
        cuda=True,
        multi_gpu=True,
        bp_update_sync=True,
        traj_len_inf=False,
        use_wandb=False,
        use_rnd_model=False,
        sampled_algo=False,
        gumbel_algo=False,
        mcts_ctree=True,
        collector_env_num=8,
        evaluator_env_num=3,
        env_type='not_board_games',
        action_type='fixed_action_space',
        battle_mode='play_with_bot_mode',
        monitor_extra_statistics=True,
        game_segment_length=20,
        eval_offline=False,
        cal_dormant_ratio=False,
        analysis_sim_norm=False,
        analysis_dormant_ratio=False,
        transform2string=False,
        gray_scale=False,
        use_augmentation=False,
        augmentation=['shift', 'intensity'],
        ignore_done=False,
        update_per_collect=80,
        replay_ratio=0.25,
        batch_size=[64, 64, 64, 64, 64, 64, 64, 64],
        optim_type='AdamW',
        learning_rate=0.0001,
        target_update_freq=100,
        target_update_freq_for_intrinsic_reward=1000,
        weight_decay=0.0001,
        momentum=0.9,
        grad_clip_value=5,
        n_episode=8,
        num_segments=8,
        num_simulations=50,
        discount_factor=0.997,
        td_steps=5,
        num_unroll_steps=10,
        reward_loss_weight=1,
        value_loss_weight=0.25,
        policy_loss_weight=1,
        policy_entropy_weight=0,
        ssl_loss_weight=0,
        piecewise_decay_lr_scheduler=False,
        threshold_training_steps_for_final_lr=50000,
        manual_temperature_decay=False,
        threshold_training_steps_for_final_temperature=100000,
        fixed_temperature_value=0.25,
        use_ture_chance_label_in_chance_encoder=False,
        reanalyze_noise=True,
        reuse_search=False,
        collect_with_pure_policy=False,
        use_priority=False,
        priority_prob_alpha=0.6,
        priority_prob_beta=0.4,
        root_dirichlet_alpha=0.3,
        root_noise_weight=0.25,
        random_collect_episode_num=0,
        eps={'eps_greedy_exploration_in_collect': False, 'type': 'linear', 'start': 1.0, 'end': 0.05, 'decay': 100000},
        cfg_type='UniZeroMTPolicyDict',
        eval_freq=10000,
        sample_type='transition',
        target_update_theta=0.05,
        cos_lr_scheduler=False,
        accumulation_steps=1,
        train_start_after_envsteps=0,
        lr_piecewise_constant_decay=False,
        import_names=['lzero.policy.unizero_multitask'],
        only_use_moco_stats=False,
        use_moco=False,
        grad_correct_params={'MoCo_beta': 0.5, 'MoCo_beta_sigma': 0.5, 'MoCo_gamma': 0.1, 'MoCo_gamma_sigma': 0.5, 'MoCo_rho': 0, 'calpha': 0.5, 'rescale': 1},
        total_task_num=8,
        task_num=4,
        task_id=0,
        use_task_exploitation_weight=False,
        target_return=20,
        balance_pipeline=True,
        task_complexity_weight=True,
        total_batch_size=512,
        allocated_batch_sizes=False,
        print_task_priority_logs=False,
        model_path=None,
        reanalyze_ratio=0.0,
        replay_buffer_size=500000,
        buffer_reanalyze_freq=1e-06,
        reanalyze_batch_size=160,
        reanalyze_partition=0.75,
        device='cuda',
    ),
)
main_config = EasyDict(main_config)
main_config = main_config
create_config = dict(
    env=dict(
        type='atari_lightzero',
        import_names=['zoo.atari.envs.atari_lightzero_env'],
    ),
    env_manager=dict(
        cfg_type='SyncSubprocessEnvManagerDict',
        type='subprocess',
    ),
    policy=dict(type='unizero_multitask'),
)
create_config = EasyDict(create_config)
create_config = create_config
